{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.300\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/homebrew/lib/python3.11/site-packages\n",
      "Requires: aiohttp, anyio, dataclasses-json, jsonpatch, langsmith, numexpr, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip3 show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain --upgrade -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# Test the .env was loaded correctly.\n",
    "# os.environ.get('PINECONE_API_KEY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Models (Wrappers): GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.7, 'max_tokens': 512, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=512)\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Quantum mechanics is a physical theory describing the behaviour of matter and energy at the atomic and subatomic levels.\n"
     ]
    }
   ],
   "source": [
    "output = llm(\"Explain quantim mechanics in one sentence\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_num_tokens(\"Explain quantim mechanics in one sentence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.generate(['... is the capital of France.', 'What is the formula for the area of a circle?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Generation(text='\\n\\nParis', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nA = πr² (A is the area of the circle, π is the constant 3.14, and r is the radius of the circle)', generation_info={'finish_reason': 'stop', 'logprobs': None})]]\n"
     ]
    }
   ],
   "source": [
    "print(output.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Paris\n"
     ]
    }
   ],
   "source": [
    "print(output.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.generate(['Write an original tagline for a burger restaurant'] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\n\"Tastes So Good, You\\'ll Flip Out!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"The burgers that will make your mouth water!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Big, Juicy, Delicious - Our Burgers Can\\'t Be Beat!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 71, 'completion_tokens': 44, 'prompt_tokens': 27}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('5fb808fc-3a69-40fe-8349-abf081f0f2c1')), RunInfo(run_id=UUID('9231bd96-a4de-4e33-9595-da77ebf68474')), RunInfo(run_id=UUID('c21f8850-70fc-49e4-bb66-078fa990488f'))]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Tastes So Good, You'll Flip Out!\"\n",
      "\n",
      "\n",
      "\"The burgers that will make your mouth water!\"\n",
      "\n",
      "\n",
      "\"Big, Juicy, Delicious - Our Burgers Can't Be Beat!\"\n"
     ]
    }
   ],
   "source": [
    "for o in output.generations:\n",
    "    print(o[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatModels: GPT-3.5-Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5, max_tokens=1024)\n",
    "messages = [\n",
    "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
    "    HumanMessage(content='Explain quantum mechanics in one sentence')\n",
    "]\n",
    "output = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantenmechanik beschreibt das Verhalten von Teilchen auf subatomarer Ebene und ermöglicht es uns, ihre Positionen und Geschwindigkeiten nicht genau vorherzusagen, sondern nur Wahrscheinlichkeiten anzugeben.\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['virus', 'language'] output_parser=None partial_variables={} template='\\n    You are an experienced virologist. Write a few sentences about the following\\n    {virus} in {language}.\\n' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "    You are an experienced virologist. Write a few sentences about the following\n",
    "    {virus} in {language}.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['virus','language'],\n",
    "    template=template\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Is é ebola an t-ainm atá ar an níos mó de na víreas a bhaineann le buaiclíneacht súl. Tá sé ina chúis do chliseadh éigeandála ar fud an domhain agus is féidir leis an víreas a bhaineann le buaiclíneacht súl críonna a bheith ina chúis le buaiteanna móra sábhála. Tá ábhartha éagsúla ann chun tocsainí a chosc ó ebola, lena n-áirítear úsáid a bhaint as cógaisíochtaí, ábhartha sláinte a choinneáil saor in aisce agus ábhartha eile a úsáid a bhaint as chun an t-ebola a chosc.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name='text-davinci-003', temperature=0.7)\n",
    "output = llm(prompt.format(virus='ebola', language='gaelic'))\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "\n",
    "template = \"\"\"\n",
    "    You are an experienced virologist. Write a few sentences about the following\n",
    "    {virus} in {language}.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['virus','language'],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "output = chain.run({'virus': 'COVID-19', 'language':'French'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID-19, également connu sous le nom de coronavirus, est une maladie respiratoire causée par le virus SARS-CoV-2. Il a été identifié pour la première fois à Wuhan, en Chine, en décembre 2019 et s'est depuis propagé dans le monde entier, devenant une pandémie. Les symptômes courants de la COVID-19 comprennent la fièvre, la toux, la fatigue et les difficultés respiratoires. Il est essentiel de suivre les mesures de prévention telles que le lavage régulier des mains, le port du masque et la distanciation sociale pour limiter la propagation du virus.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential chains\n",
    "* Make a series of calls to one or more LLMs. Take the output from one chain and use it as the input to another chain.\n",
    "\n",
    "Two types:\n",
    "1. **SimpleSequentialChain** - series of chains, where each individual chain has a single input and isngle output, and the output of one step is used as input to the next.\n",
    "2. General form of sequential chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "def softmax(x):\n",
      "    \"\"\"Computes the softmax values for each value in x.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    x : array-like\n",
      "        Input values\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    array-like\n",
      "        Softmax values\n",
      "    \"\"\"\n",
      "    # Calculate the exponentials of each value in x\n",
      "    exp_x = np.exp(x)\n",
      "    \n",
      "    # Calculate the sum of the exponentials\n",
      "    sum_exp_x = np.sum(exp_x)\n",
      "    \n",
      "    # Calculate the softmax values by dividing each exponential value by the sum of the exponentials\n",
      "    softmax_values = exp_x/sum_exp_x\n",
      "    \n",
      "    return softmax_values\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe given Python code defines a function named \"softmax\" which computes the softmax values for each value in the input array \"x\". \n",
      "\n",
      "The function takes a single parameter \"x\". It is expected that \"x\" should be an array-like object containing the input values.\n",
      "\n",
      "The first step of the function is to calculate the exponential of each value in the input array using the \"np.exp\" method from the NumPy library. This step is performed to obtain the numerators for the softmax computation.\n",
      "\n",
      "Next, the function calculates the sum of all exponential values using the \"np.sum\" method from NumPy. This step is necessary to obtain the denominator for the softmax computation.\n",
      "\n",
      "Then, the softmax values are computed by dividing each exponential value by the sum of all exponential values. This is done element-wise using the \"/\" operator which automatically broadcasts the division operation for each value in the arrays.\n",
      "\n",
      "Finally, the function returns the softmax values produced as a result of the computation.\n",
      "\n",
      "Overall, the softmax function applies the softmax activation function to each value in the input array \"x\", providing a probability distribution where the values are scaled to represent the likelihood of each class based on their corresponding score magnitude.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=1024)\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=['concept'],\n",
    "    template=\"\"\"You are an experienced scientist and Python programmer.\n",
    "    Write a function that implements the concept of {concept}.\"\"\"\n",
    ")\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt1)\n",
    "\n",
    "\n",
    "llm2 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=['function'],\n",
    "    template='Given the Python {function}, describe it as detailed as possible.'\n",
    ")\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "output = overall_chain.run('softmax')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Agents\n",
    "* LLMs like ChatGPT cannot do math very well because they need to use approximations\n",
    "* They also can't browse the internet, genuinely run database queries, etc.\n",
    "* LangChain Agents can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I need to calculate the factorial of 20 and then take the square root of that number.\n",
      "Action: Python_REPL\n",
      "Action Input: from math import factorial; print(round(factorial(20)**0.5, 4))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m1559776268.6285\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 1559776268.6285\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1559776268.6285'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),  # functions that agents can use to interact with the outside world\n",
    "    verbose=True  # so we can see the intermediate text\n",
    ")\n",
    "agent_executor.run('Calculate the square root of the factorial of 20 \\\n",
    "                   and display it with four decimal points.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to use the Python REPL to calculate the answer\n",
      "Action: Python_REPL\n",
      "Action Input: print(\"{:,.2f}\".format(5.1 ** 7.3))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m146,306.05\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 146,306.05\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'146,306.05'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run('what is the answer to 5.1 ** 7.3? Format the output for read-ability with commas and two decimals.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "* Embeddings are the core of LLM applications.\n",
    "* Text embeddings are numeric representationa os text and are used in NLP and ML tasks.\n",
    "* The *distance* between two embeddings or two vectors *measures their relatedness* which translates to the relatedness between the text concepts they represent.\n",
    "* Similar embeddings or vectors represent similar concepts.\n",
    "\n",
    "**Embeddings Applications**\n",
    "* *Text Classification*: assigning a label to a piece of text.\n",
    "* *Text Clusting*: grouping together pieces of text that are similar in meaning.\n",
    "* *Question Answering*: answering a question posed in natural language."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Databases\n",
    "\n",
    "**AI Challenges**\n",
    "* Efficient data processing (AI uses huge amounts of data)\n",
    "* Many of the latest AI applications rely on vector embeddings. Chatbots, question-answering systems, and machine translation rely on vector embeddings.\n",
    "\n",
    "**Vector Databases**\n",
    "\n",
    "A new type of database designed to store and query unstructured data.\n",
    "* *Pinecone*: Which we are using in this project.\n",
    "* *chroma*\n",
    "* *milvus*\n",
    "* *drant*\n",
    "\n",
    "**SQL vs. Vector Database**\n",
    "\n",
    "* SQL is structured, but when querying a vector database the query looks for the most similar item.\n",
    "* Vector databases use a combination of different optimized algorithms that all participate in **Approximage Nearest Neighbor (ANN)** search.\n",
    "Steps:\n",
    "1. *Embedding*: create vector embeddings for the content we want to index.\n",
    "2. *Indexing*: insert the vector embedding into the vector database.\n",
    "3. *Querying*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and Embedding Text Using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "with open('./churchill_speech.txt') as f:\n",
    "    churchill_speech = f.read()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,  # help maintain continuity\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The position of the B. E.F had now become critical As a result of a most skillfully conducted\n",
      "Now you have 281 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunks = text_splitter.create_documents([churchill_speech])\n",
    "print(chunks[1].page_content)\n",
    "print(f'Now you have {len(chunks)} chunks.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 5629\n",
      "Embedding cost in USD: $0.002252\n"
     ]
    }
   ],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding cost in USD: ${total_tokens / 1000 * 0.0004:.6f}')\n",
    "\n",
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.02832016386419352,\n",
       " -0.0036371849028618967,\n",
       " -0.03352817192461715,\n",
       " -0.03601557809223016,\n",
       " -0.012896945871190453,\n",
       " -0.0003635565274675487,\n",
       " 0.006833891474988558,\n",
       " -0.004553768378574473,\n",
       " -0.013305035596318493,\n",
       " -0.0009651656856314292]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = embedding.embed_query(chunks[0].page_content)\n",
    "vector[0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting the Embeddings into a Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all Pinecone indices.Done\n"
     ]
    }
   ],
   "source": [
    "# Create a pinecone index (or use an existing)\n",
    "# Free version is limited to 1 index, so we need to delete any before making a new one.\n",
    "indexes = pinecone.list_indexes()\n",
    "for i in indexes:\n",
    "    print(\"Deleting all Pinecone indices.\", end=\"\")\n",
    "    pinecone.delete_index(i)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index churchill-speech ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "index_name = 'churchill-speech'\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    print(f\"Creating index {index_name} ...\")\n",
    "    pinecone.create_index(index_name, dimension=1536, metric='cosine')  # OpenAI dimension defaults to 1,536\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Pinecone.from_documents(chunks, embedding, index_name=index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking Questions (Similarity Seach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='on the beaches, we shall fight on the landing grounds, we shall fight in the fields and in the', metadata={}), Document(page_content='fields and in the streets, we shall fight in the hills; we shall never surrender, and even if,', metadata={}), Document(page_content='When we consider how much greater would be our advantage in defending the air above this Island', metadata={}), Document(page_content='front, now on that, fighting on three fronts at once, battles fought by two or three divisions', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "query = 'Where should we fight?'\n",
    "result = vector_store.similarity_search(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the beaches, we shall fight on the landing grounds, we shall fight in the fields and in the\n",
      "--------------------------------------------------\n",
      "fields and in the streets, we shall fight in the hills; we shall never surrender, and even if,\n",
      "--------------------------------------------------\n",
      "When we consider how much greater would be our advantage in defending the air above this Island\n",
      "--------------------------------------------------\n",
      "front, now on that, fighting on three fronts at once, battles fought by two or three divisions\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for r in result:\n",
    "    print(r.page_content)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type='similarity',search_kwargs={'k': 3})  # the 3 most similar chunks\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)  #stuff is the default and uses all of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The king of Belgium at the time was not mentioned in the provided context.\n"
     ]
    }
   ],
   "source": [
    "# query = \"Where should we fight?\"\n",
    "query = 'Who was the king of Belgium at the time?'\n",
    "answer = chain.run(query)\n",
    "print(answer)  # I guess I didn't get the correct speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We should fight on the beaches, the landing grounds, in the fields, streets, and hills.'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Where should we fight?\"\n",
    "chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The French Armies played a significant role in the context mentioned. They were part of the conflict with the British Armies and were involved in the efforts to reopen communications to Amiens. Additionally, a French Army was planned to advance across the Somme in order to secure strategic positions.\n"
     ]
    }
   ],
   "source": [
    "query = \"What about the French Armies?\"\n",
    "answer = chain.run(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The French Armies were a significant force that played a crucial role in the military operations. They were involved in the fighting against the British Armies and their main objective was to reopen communications to Amiens. Additionally, a French Army was created specifically to advance across the Somme and seize control of important territories.\n"
     ]
    }
   ],
   "source": [
    "query = \"What about the French Armies?\"\n",
    "answer = chain.run(query)\n",
    "print(answer)  # The answers vary as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Obtaining dependency information for pypdf from https://files.pythonhosted.org/packages/bf/53/8840f93c5dcd108c02cac7343e194f9dc5d15ade6200ccc661ab4e1352b5/pypdf-3.16.2-py3-none-any.whl.metadata\n",
      "  Downloading pypdf-3.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Downloading pypdf-3.16.2-py3-none-any.whl (276 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-3.16.2\n"
     ]
    }
   ],
   "source": [
    "# Let's see if we can read the player's handbook for D&D 5e.\n",
    "!pip3 install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./fifth_player_handbook.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Rogues devote as much effort to mastering the use of a variety of skills as they do to perfecting their com bat abilities, giving them a broad expertise that few other characters can match. Many rogues focus on stealth and deception, while others refine the skills that help them in a dungeon environment, such as climbing, finding and disarming traps, and opening locks.When i t  comes to combat, rogues prioritize cunning over brute strength. A  rogue w ould rather make one precise strike, placing it exactly where the attack will hurt the target most, than w ear an opponent down with a barrage of attacks. R ogues have an almost supernatural knack for avoiding danger, and a few learn magical tricks to supplement their other abilities.A Shady Liv in gEvery town and city has its share of rogues. M ost of them live up to the worst stereotypes of the class, making a living as burglars, assassins, cutpurses, and con artists. Often, these scoundrels are organized into thieves’ guilds or crim e families. Plenty of rogues operate independently, but even they som etim es recruit apprentices to help them in their scams and heists. A few rogues make an honest living asRogue\\nSki ll and Pre ci s i onSignaling for her com panions to wait, a halfling creeps forward through the dungeon hall. She presses an ear to the door, then pulls out a set of tools and picks the lock in the blink of an eye. Then she disappears into the shadows as her fighter friend m oves forward to kick the door open.A human lurks in the shadows of an alley while his accom plice prepares for her part in the ambush. W hen their target—a notorious slaver—passes the alleyway, the accomplice cries out, the slaver com es to investigate, and the assassin’s blade cuts his throat before he can make a sound.Suppressing a giggle, a gnom e w aggles her fingers and magically lifts the key ring from the guard’s belt.In a moment, the keys are in her hand, the cell door is open, and she and her companions are free to make their escape.R ogues rely on skill, stealth, and their foes’ vulnerabilities to get the upper hand in any situation. They have a knack for finding the solution to just about any problem, demonstrating a resourcefulness and versatility that is the cornerstone of any successful adventuring party.', metadata={'source': './fifth_player_handbook.pdf', 'page': 87})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
